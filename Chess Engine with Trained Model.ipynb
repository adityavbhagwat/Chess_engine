{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586773d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-chess==0.31.3 in c:\\users\\adibh\\appdata\\roaming\\python\\python310\\site-packages (0.31.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user python-chess==0.31.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf442d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "#we will generate test dataset by creating random chess positions, and then using some external evaluator to get the evaluation number\n",
    "def random_board(max_moves=200):\n",
    "  board = chess.Board()\n",
    "  depth = random.randrange(0,max_moves)\n",
    "  for _ in range(depth):\n",
    "    possible_moves = list(board.legal_moves)\n",
    "    random_move = random.choice(possible_moves)\n",
    "    board.push(random_move)\n",
    "    if(board.is_game_over()):\n",
    "      break\n",
    "  return board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "370f94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this path to the path where stockfish is installed in ur system\n",
    "stockfish_path = r\"C:\\Users\\adibh\\stockfish\\stockfish-windows-x86-64-avx2.exe\"\n",
    "\n",
    "# Initialize the engine\n",
    "def stockfish(board,depth):\n",
    "  with chess.engine.SimpleEngine.popen_uci(stockfish_path) as sf:\n",
    "    result = sf.analyse(board, chess.engine.Limit(depth=depth))\n",
    "    score = result['score'].white().score()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c1dfca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38-1.95 1.12-3.28 3.21-3.28 5.62 0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark lastmove a1\" stroke=\"none\" fill=\"#aaa23b\" /><use xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light lastmove b1\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-knight\" transform=\"translate(15, 240)\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-queen\" transform=\"translate(105, 240)\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-pawn\" transform=\"translate(240, 240)\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-king\" transform=\"translate(330, 240)\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#black-pawn\" transform=\"translate(150, 195)\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-pawn\" transform=\"translate(195, 195)\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#white-pawn\" transform=\"translate(285, 195)\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-pawn\" transform=\"translate(60, 150)\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-pawn\" transform=\"translate(105, 150)\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-pawn\" transform=\"translate(150, 150)\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#white-bishop\" transform=\"translate(285, 150)\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-pawn\" transform=\"translate(330, 150)\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#black-pawn\" transform=\"translate(60, 105)\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-knight\" transform=\"translate(105, 105)\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-bishop\" transform=\"translate(150, 60)\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-king\" transform=\"translate(105, 15)\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#black-bishop\" transform=\"translate(150, 15)\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><use xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g></svg>"
      ],
      "text/plain": [
       "Board('r1kb2nr/3b4/1pn5/1pPp2Bp/3pp1P1/N1Q2P1K/P1P3P1/R5NR b - - 4 27')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = random_board()\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b10a924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r . k b . . n r\n",
      ". . . b . . . .\n",
      ". p n . . . . .\n",
      ". p P p . . B p\n",
      ". . . p p . P .\n",
      "N . Q . . P . K\n",
      "P . P . . . P .\n",
      "R . . . . . N R\n",
      "-554\n"
     ]
    }
   ],
   "source": [
    "#just testing it out, clearly white is better\n",
    "print(board)\n",
    "print(stockfish(board,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "380278be",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares_index = {\n",
    "    'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7\n",
    "}\n",
    "def square_to_index(square):\n",
    "    letter = chess.square_name(square)\n",
    "    return 8-int(letter[1]),squares_index[letter[0]]\n",
    "def board_to_array(board):\n",
    "    boardarr = numpy.zeros((14,8,8),dtype=numpy.int32)\n",
    "#     boardarr[0] --> piece type\n",
    "#     boardarr[1],[2] --> cell location\n",
    "    for piece in chess.PIECE_TYPES: #this generates all types of pieces. each piece is denoted by some numerical value\n",
    "        for square in board.pieces(piece,chess.WHITE):\n",
    "            #this gives list of all squares where the needed piece is present\n",
    "            idx = numpy.unravel_index(square,(8,8))\n",
    "            boardarr[piece-1][7-idx[0]][idx[1]] = 1 #white piece info in the range boardarr[0] to boardarr[5]\n",
    "        for square in board.pieces(piece,chess.BLACK):\n",
    "            #this gives list of all squares where the needed piece is present\n",
    "            idx = numpy.unravel_index(square,(8,8))\n",
    "            boardarr[piece+5][7-idx[0]][idx[1]] = 1 #black piece info in the range boardarr[6] to boardarr[11]\n",
    "    #we will add 2 extra dimensions , in each dimension we will mark 1 at some x,y if that position is attack by any piece of opponent\n",
    "    cp = board.turn\n",
    "    board.turn = chess.WHITE\n",
    "    for move in board.legal_moves:\n",
    "        i,j = square_to_index(move.to_square)\n",
    "        boardarr[12][i][j] = 1\n",
    "    board.turn = chess.BLACK\n",
    "    for move in board.legal_moves:\n",
    "        i,j = square_to_index(move.to_square)\n",
    "        boardarr[13][i][j] = 1\n",
    "    board.turn = cp\n",
    "    return boardarr\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1909e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(board_to_array(board))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b0d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ee63fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as model\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "def build_model(conv_size,conv_depth):\n",
    "    board3d = layers.Input(shape=(14,8,8))\n",
    "    x = board3d\n",
    "    for _ in range(conv_depth):\n",
    "        x = layers.Conv2D(filters=conv_size,kernel_size = 3,padding='same',activation='relu',input_shape=(14,8,8))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64,'relu')(x)\n",
    "    x = layers.Dense(1,'sigmoid')(x)\n",
    "    return tensorflow.keras.models.Model(inputs=board3d,outputs =x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e3e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 8, 32)         2336      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3584)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                229440    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259,585\n",
      "Trainable params: 259,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chess_model = build_model(32,4)\n",
    "chess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d38d13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\adibh\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-2.0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43bd258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "     -------------------------------------- 47.1/47.1 kB 801.9 kB/s eta 0:00:00\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e882818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as model\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "def build_model_residual(conv_size, conv_depth):\n",
    "    board3d = layers.Input(shape=(14,8,8))\n",
    "    x = layers.Conv2D(filters=conv_size,kernel_size=3,padding='same')(board3d)\n",
    "    for _ in range(conv_depth):\n",
    "        previous = x\n",
    "        \n",
    "        x = layers.Conv2D(filters=conv_size,kernel_size = 3,padding='same',activation='relu',input_shape=(14,8,8))(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters=conv_size,kernel_size = 3,padding='same',activation='relu',input_shape=(14,8,8))(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x=layers.Activation('relu')(x)\n",
    "        x=layers.Add()([x,previous])\n",
    "        x=layers.Activation('relu')(x)       \n",
    "        \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1,'sigmoid')(x)\n",
    "    return model.Model(inputs=board3d,outputs =x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3560d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 14, 8, 8)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 14, 8, 32)    2336        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 14, 8, 32)    9248        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 14, 8, 32)   128         ['conv2d_5[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 14, 8, 32)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 14, 8, 32)    9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 14, 8, 32)    0           ['activation_1[0][0]',           \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 14, 8, 32)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 14, 8, 32)    9248        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 14, 8, 32)    9248        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 14, 8, 32)    0           ['activation_4[0][0]',           \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 14, 8, 32)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 14, 8, 32)    9248        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 8, 32)    9248        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 14, 8, 32)    0           ['activation_7[0][0]',           \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 14, 8, 32)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 14, 8, 32)    9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 14, 8, 32)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 14, 8, 32)    9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 14, 8, 32)   128         ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 14, 8, 32)    0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 14, 8, 32)    0           ['activation_10[0][0]',          \n",
      "                                                                  'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 14, 8, 32)    0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 3584)         0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            3585        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,929\n",
      "Trainable params: 80,417\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "res_model = build_model_residual(32,4)\n",
    "res_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4f71049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14, 8, 8)   (10,)\n",
      "2.9315831661224365\n"
     ]
    }
   ],
   "source": [
    "#let us now generate 100k random boards and store it in x_train,y_train\n",
    "import time\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Code to be measured\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no_samples = 10\n",
    "x_elements = []\n",
    "y_elements = []\n",
    "for _ in range(no_samples):\n",
    "    board = random_board()\n",
    "    y_tag = stockfish(board,4)\n",
    "    x_tag = board_to_array(board)\n",
    "    x_elements.append(x_tag)\n",
    "    y_elements.append(y_tag)\n",
    "result_x = numpy.concatenate(x_elements, axis=0).reshape(no_samples, 14,8,8)\n",
    "result_y = numpy.array(y_elements)\n",
    "result_y = result_y/abs(result_y.max())/2+0.5\n",
    "print(result_x.shape,\" \",result_y.shape)\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b948d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 8, 32)         2336      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 8, 32)         9248      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3584)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                229440    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259,585\n",
      "Trainable params: 259,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 1s 27ms/step - loss: 0.0813 - val_loss: 0.1571\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0696 - val_loss: 0.1516\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0565 - val_loss: 0.1389\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0369 - val_loss: 0.1446\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0234 - val_loss: 0.1229\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0202 - val_loss: 0.1222\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.2267\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0075 - val_loss: 0.1727\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0033 - val_loss: 0.1577\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0022 - val_loss: 0.1945\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0012 - val_loss: 0.1941\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5.2723e-04 - val_loss: 0.1986\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5.2117e-04 - val_loss: 0.2076\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 4.4477e-04 - val_loss: 0.2088\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 3.5531e-04 - val_loss: 0.1990\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 3.6719e-04 - val_loss: 0.2124\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.7574e-04 - val_loss: 0.1938\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 4.3610e-04 - val_loss: 0.2212\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.7170e-04 - val_loss: 0.1854\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 3.7207e-04 - val_loss: 0.2275\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 0.1526\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0023 - val_loss: 0.1909\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0018 - val_loss: 0.1869\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 5.4571e-04 - val_loss: 0.1809\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5.0175e-04 - val_loss: 0.2055\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6.7867e-04 - val_loss: 0.1683\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 9.1652e-04 - val_loss: 0.2206\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 7.7974e-04 - val_loss: 0.1850\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 5.4956e-04 - val_loss: 0.1936\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5.0220e-04 - val_loss: 0.1911\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 4.4927e-04 - val_loss: 0.1733\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.9506e-04 - val_loss: 0.2167\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.1825\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0013 - val_loss: 0.2058\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0019 - val_loss: 0.1923\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0022 - val_loss: 0.1492\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0012 - val_loss: 0.1872\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.0752e-04 - val_loss: 0.2060\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.3598e-04 - val_loss: 0.1937\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 2.3915e-04 - val_loss: 0.2046\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 2.7024e-04 - val_loss: 0.2019\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 2.0597e-04 - val_loss: 0.2070\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 1.1373e-04 - val_loss: 0.2019\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 9.5930e-05 - val_loss: 0.2038\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8.7144e-05 - val_loss: 0.2060\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 7.3996e-05 - val_loss: 0.2027\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6.2095e-05 - val_loss: 0.2055\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5.2448e-05 - val_loss: 0.2032\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5.0451e-05 - val_loss: 0.2077\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5.2673e-05 - val_loss: 0.2050\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 4.8746e-05 - val_loss: 0.2052\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 4.3694e-05 - val_loss: 0.2083\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.8431e-05 - val_loss: 0.2060\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.4889e-05 - val_loss: 0.2086\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 3.2781e-05 - val_loss: 0.2061\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.2119e-05 - val_loss: 0.2089\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 3.0808e-05 - val_loss: 0.2071\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 3.3282e-05 - val_loss: 0.2113\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 4.0270e-05 - val_loss: 0.2083\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5.6103e-05 - val_loss: 0.2117\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 7.0182e-05 - val_loss: 0.2086\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 5.0819e-05 - val_loss: 0.2122\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 3.6424e-05 - val_loss: 0.2060\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 4.4580e-05 - val_loss: 0.2204\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 1.1170e-04 - val_loss: 0.2094\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.8089e-04 - val_loss: 0.2152\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 3.9339e-04 - val_loss: 0.1925\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 4.3840e-04 - val_loss: 0.2177\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.2386e-04 - val_loss: 0.1946\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 1.6573e-04 - val_loss: 0.2205\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 2.2583e-04 - val_loss: 0.1961\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1.8959e-04 - val_loss: 0.2084\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 1.7367e-04 - val_loss: 0.1982\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 2.3643e-04 - val_loss: 0.2110\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 2.9017e-04 - val_loss: 0.2020\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 2.5550e-04 - val_loss: 0.2109\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 3.5976e-04 - val_loss: 0.1943\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6.6393e-04 - val_loss: 0.2066\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0013 - val_loss: 0.2075\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.2036\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5.3828e-04 - val_loss: 0.2013\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 4.3170e-04 - val_loss: 0.1985\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 2.0560e-04 - val_loss: 0.2138\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 1.0070e-04 - val_loss: 0.2034\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.1968e-05 - val_loss: 0.2123\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 8.9938e-05 - val_loss: 0.2084\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 1.5644e-04 - val_loss: 0.2096\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 3.3814e-04 - val_loss: 0.2119\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 2.0349e-04 - val_loss: 0.1970\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.2211\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.8017e-04 - val_loss: 0.1813\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0013 - val_loss: 0.2108\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 0.2347\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6.2714e-04 - val_loss: 0.2064\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4.2241e-04 - val_loss: 0.2030\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7.2408e-04 - val_loss: 0.2161\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 3.7858e-04 - val_loss: 0.2069\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 1.6853e-04 - val_loss: 0.2115\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.4110e-04 - val_loss: 0.2072\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 7.7677e-05 - val_loss: 0.2156\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('model.h5', save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "chess_model.compile(optimizer = optimizers.Adam(5e-4),loss='mean_squared_error')\n",
    "chess_model.summary()\n",
    "chess_model.fit(result_x,result_y,batch_size=1,epochs=100,verbose=1,validation_split=0.1)\n",
    "chess_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddb14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
